<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ç™¾äººä¸€é¦–ã‚«ãƒ¼ãƒ‰åˆ¤åˆ¥ãƒ‡ãƒ¢</title>
  <style>
    body {
      font-family: sans-serif;
      text-align: center;
      padding: 1rem;
    }

    video,
    canvas {
      display: block;
      margin: 0 auto;
    }

    #result {
      font-size: 1.5rem;
      margin-top: 1rem;
    }

    #debug {
      font-size: 0.8rem;
      color: #666;
      text-align: left;
      white-space: pre-wrap;
      margin: 1rem auto;
      max-width: 90%;
    }
  </style>
</head>

<body>
  <h1>ç™¾äººä¸€é¦–ã‚«ãƒ¼ãƒ‰åˆ¤åˆ¥ãƒ‡ãƒ¢</h1>
  <video id="video" playsinline autoplay muted></video>
  <canvas id="canvas" style="display:none;"></canvas>
  <div id="result">èª­ã¿å–ã‚Šä¸­â€¦</div>
  <div id="debug">ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼š</div>

  <!-- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”»åƒï¼ˆéš ã—è¦ç´ ï¼‰-->
  <div style="display:none">
    <!-- ä»¥ä¸‹ã¯ template/1.jpg ã€œ template/100.jpg ã‚’æƒ³å®š. è»½é‡åŒ–ã®ãŸã‚ä¸€æ—¦5å€‹ -->
    <img id="tpl1" src="template/1.jpg" alt="1ç•ª" />
    <img id="tpl2" src="template/2.jpg" alt="2ç•ª" />
    <img id="tpl3" src="template/3.jpg" alt="3ç•ª" />
    <img id="tpl4" src="template/4.jpg" alt="4ç•ª" />
    <img id="tpl5" src="template/5.jpg" alt="5ç•ª" />
  </div>

  <!-- OpenCV.js -->
  <script src="https://docs.opencv.org/4.12.0/opencv.js"></script>
  <script>
    console.log("ğŸ±â€ğŸ’» ã‚¹ã‚¯ãƒªãƒ—ãƒˆèª­ã¿è¾¼ã¿ OK");

    // ã‚°ãƒ­ãƒ¼ãƒãƒ«å®£è¨€
    let video, resultEl, debugEl, capCanvas, capCtx;
    let orb, bf;
    const TEMPLATE_COUNT = 100;
    const templates = [];

    // ã‚«ãƒ¡ãƒ©ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'environment' }
      });
      video.srcObject = stream;
      return new Promise(r => video.onloadedmetadata = () => r());
    }

    // ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    function loadTemplates() {
      debugEl.textContent = 'Step: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ä¸­â€¦';
      for (let i = 1; i <= TEMPLATE_COUNT; i++) {
        const img = document.getElementById(`tpl${i}`);
        if (!img) continue;
        const mat = cv.imread(img);
        const gray = new cv.Mat();
        cv.cvtColor(mat, gray, cv.COLOR_RGBA2GRAY);
        const kp = new cv.KeyPointVector();
        const des = new cv.Mat();
        orb.detectAndCompute(gray, new cv.Mat(), kp, des);
        templates.push({ id: i, descriptors: des });
        mat.delete(); gray.delete();
      }
      debugEl.textContent = `Step: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ${templates.length} æšèª­ã¿è¾¼ã¿å®Œäº†`;
    }

    // å››è§’å½¢æ¤œå‡ºï¼‹é€è¦–å¤‰æ›
    function detectCardROI(srcGray) {
      debugEl.textContent = 'Step: è¼ªéƒ­æ¤œå‡ºä¸­â€¦';
      const contours = new cv.MatVector();
      const hier = new cv.Mat();
      cv.findContours(srcGray, contours, hier, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE);
      for (let i = 0; i < contours.size(); i++) {
        const cnt = contours.get(i);
        const approx = new cv.Mat();
        cv.approxPolyDP(cnt, approx, 5, true);
        if (approx.rows === 4 && cv.contourArea(approx) > 10000) {
          debugEl.textContent = 'Step: é€è¦–å¤‰æ›ä¸­â€¦';
          // é ‚ç‚¹ã‚½ãƒ¼ãƒˆ
          const pts = [];
          for (let j = 0; j < 4; j++) {
            pts.push({ x: approx.intPtr(j, 0)[0], y: approx.intPtr(j, 0)[1] });
          }
          pts.sort((a, b) => a.y - b.y);
          const top = pts.slice(0, 2).sort((a, b) => a.x - b.x);
          const bot = pts.slice(2, 4).sort((a, b) => a.x - b.x);
          const srcPts = cv.matFromArray(4, 1, cv.CV_32FC2, [
            top[0].x, top[0].y, top[1].x, top[1].y,
            bot[1].x, bot[1].y, bot[0].x, bot[0].y
          ]);
          const w = 300, h = 450;
          const dstPts = cv.matFromArray(4, 1, cv.CV_32FC2, [0, 0, w, 0, w, h, 0, h]);
          const M = cv.getPerspectiveTransform(srcPts, dstPts);
          const warped = new cv.Mat();
          cv.warpPerspective(srcGray, warped, M, new cv.Size(w, h));
          // è§£æ”¾
          cnt.delete(); approx.delete();
          contours.delete(); hier.delete();
          srcPts.delete(); dstPts.delete(); M.delete();
          return warped;
        }
        approx.delete(); cnt.delete();
      }
      contours.delete(); hier.delete();
      return null;
    }

    // ORB ãƒãƒƒãƒãƒ³ã‚°
    function computeBestMatch(warpedGray) {
      debugEl.textContent = 'Step: ç‰¹å¾´é‡æŠ½å‡ºä¸­â€¦';
      const kp1 = new cv.KeyPointVector();
      const des1 = new cv.Mat();
      orb.detectAndCompute(warpedGray, new cv.Mat(), kp1, des1);

      debugEl.textContent = 'Step: ãƒãƒƒãƒãƒ³ã‚°ä¸­â€¦';
      let best = { id: null, count: 0 };
      for (const tpl of templates) {
        const matches = new cv.DMatchVectorVector();
        bf.knnMatch(des1, tpl.descriptors, matches, 2);
        let good = 0;
        for (let i = 0; i < matches.size(); i++) {
          const m = matches.get(i).get(0), n = matches.get(i).get(1);
          if (m.distance < 0.75 * n.distance) good++;
        }
        matches.delete();
        if (good > best.count) best = { id: tpl.id, count: good };
      }
      des1.delete(); kp1.delete();
      return best;
    }

    // ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†
    function processFrame() {
      debugEl.textContent = 'Step: ã‚­ãƒ£ãƒ—ãƒãƒ£ä¸­â€¦';
      capCtx.drawImage(video, 0, 0, capCanvas.width, capCanvas.height);

      debugEl.textContent = 'Step: Mat å¤‰æ›ä¸­â€¦';
      const src = cv.imread(capCanvas);
      const gray = new cv.Mat();
      cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY);

      debugEl.textContent = 'Step: ROIæ¤œå‡ºé–‹å§‹â€¦';
      const roi = detectCardROI(gray);

      if (roi) {
        const match = computeBestMatch(roi);
        debugEl.textContent = `Step: ãƒãƒƒãƒå®Œäº† â†’ id=${match.id} (count=${match.count})`;
        resultEl.textContent = match.id ? `æ­Œç•ªå·ï¼š${match.id}` : 'æœªæ¤œå‡º';
        roi.delete();
      } else {
        debugEl.textContent = 'Step: ROIæœªæ¤œå‡º â†’ ã‚«ãƒ¼ãƒ‰æœªæ¤œå‡º';
        resultEl.textContent = 'ã‚«ãƒ¼ãƒ‰æœªæ¤œå‡º';
      }

      src.delete(); gray.delete();
      requestAnimationFrame(processFrame);
    }

    // DOM æº–å‚™ã¨ OpenCV åˆæœŸåŒ–å¾…ã¡
    document.addEventListener('DOMContentLoaded', async () => {
      // è¦ç´ å–å¾—
      video = document.getElementById('video');
      resultEl = document.getElementById('result');
      debugEl = document.getElementById('debug');
      debugEl.textContent = 'Step: DOMContentLoaded å®Œäº†';

      // ã‚ªãƒ•ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚­ãƒ£ãƒ³ãƒã‚¹ä½œæˆ
      capCanvas = document.createElement('canvas');
      capCanvas.width = video.videoWidth;
      capCanvas.height = video.videoHeight;
      capCtx = capCanvas.getContext('2d');

      // ã‚«ãƒ¡ãƒ©èµ·å‹•
      await setupCamera();

      // OpenCV.js åˆæœŸåŒ–å¾…ã¡
      cv['onRuntimeInitialized'] = () => {
        debugEl.textContent = 'Step: OpenCV åˆæœŸåŒ–å®Œäº†';
        orb = new cv.ORB();
        bf = new cv.BFMatcher(cv.NORM_HAMMING);
        loadTemplates();
        processFrame();
      };
    });
  </script>
</body>

</html>